# main.py - Medical RAG API
import os
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List
import vertexai
from vertexai.generative_models import GenerativeModel
import pinecone
import httpx
import traceback
import asyncio

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
gemini_model = None
pinecone_index = None
pinecone_client = None
INITIALIZATION_ERROR = None

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
PROJECT_ID = os.environ.get("PROJECT_ID", "ai-project-26082025")
REGION = os.environ.get("REGION", "us-central1")
PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "medical-knowledge")
YANDEX_API_KEY = os.environ.get("YANDEX_API_KEY")
YANDEX_FOLDER_ID = os.environ.get("YANDEX_FOLDER_ID")
YANDEX_GPT_MODEL_URI = f"gpt://{YANDEX_FOLDER_ID}/yandexgpt/latest" if YANDEX_FOLDER_ID else None
MAX_CONTEXT_LENGTH = int(os.environ.get("MAX_CONTEXT_LENGTH", 5000))

# --- Lifespan handler ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global gemini_model, pinecone_index, pinecone_client, INITIALIZATION_ERROR
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vertex AI
        logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vertex AI: project={PROJECT_ID}, location={REGION}")
        vertexai.init(project=PROJECT_ID, location=REGION)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Gemini
        logger.info("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Gemini...")
        try:
            gemini_model = GenerativeModel("gemini-1.5-pro")
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å Gemini –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Gemini: {e}")
            gemini_model = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pinecone
        if PINECONE_API_KEY and PINECONE_INDEX_NAME:
            logger.info("üîó –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pinecone...")
            pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY)
            pinecone_index = pinecone_client.Index(PINECONE_INDEX_NAME)
            logger.info(f"‚úÖ Pinecone –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ò–Ω–¥–µ–∫—Å: {PINECONE_INDEX_NAME}")
        else:
            logger.warning("‚ö†Ô∏è Pinecone –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        INITIALIZATION_ERROR = None

    except Exception as e:
        error_msg = f"üí• –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}"
        logger.error(error_msg)
        logger.error(f"   Traceback: {traceback.format_exc()}")
        INITIALIZATION_ERROR = str(e)

    logger.info("üü¢ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–æ–≤.")
    yield
    logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")

# --- –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
app = FastAPI(
    lifespan=lifespan,
    title="Medical RAG API",
    version="1.0.0"
)

# --- Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö ---
class QuestionRequest(BaseModel):
    question: str
    mode: str = "knowledge_base"

class AnswerResponse(BaseModel):
    question: str
    answer: str
    sources: List[str]
    mode: str

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
async def search_knowledge_base(question: str, top_k: int = 3):
    """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π Pinecone"""
    global pinecone_index, pinecone_client

    if not pinecone_index or not pinecone_client:
        logger.warning("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return ["–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"], ["–°–∏—Å—Ç–µ–º–∞"]

    try:
        logger.debug(f"üîç –ü–æ–∏—Å–∫: '{question}'")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        embedding_response = await asyncio.to_thread(
            pinecone_client.inference.embed,
            model="llama-text-embed-v2",
            inputs=[question],
            parameters={"input_type": "query", "truncate": "END"}
        )

        question_embedding = embedding_response.data[0].values

        # –ü–æ–∏—Å–∫
        search_results = await asyncio.to_thread(
            pinecone_index.query,
            vector=question_embedding,
            top_k=top_k,
            include_metadata=True
        )

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        contexts = []
        sources = []

        if search_results.matches:
            for match in search_results.matches:
                metadata = match.metadata or {}
                text = metadata.get('content') or f"–î–æ–∫—É–º–µ–Ω—Ç ID: {match.id}"
                contexts.append(text)
                source = metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
                sources.append(source)
        else:
            contexts.append("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.")
            sources.append("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")

        return contexts, sources

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return ["–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞"], ["–û—à–∏–±–∫–∞"]

async def generate_with_gemini(question: str, contexts: List[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Google Gemini"""
    if not gemini_model:
        return "Google Gemini –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"

    try:
        context_text = "\n\n".join(contexts)
        if len(context_text) > MAX_CONTEXT_LENGTH:
            context_text = context_text[:MAX_CONTEXT_LENGTH] + "..."

        prompt = f"""
        –¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_text}
        –í–æ–ø—Ä–æ—Å: {question}
        –û—Ç–≤–µ—Ç:
        """.strip()

        response = gemini_model.generate_content(prompt)
        return response.text.strip()

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}")
        return f"–û—à–∏–±–∫–∞ Gemini: {str(e)}"

async def generate_with_yandex(question: str, contexts: List[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Yandex GPT"""
    if not YANDEX_API_KEY or not YANDEX_FOLDER_ID:
        return "Yandex GPT –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"

    try:
        context_text = "\n\n".join(contexts)
        if len(context_text) > MAX_CONTEXT_LENGTH:
            context_text = context_text[:MAX_CONTEXT_LENGTH] + "..."

        prompt = f"""
        –¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_text}
        –í–æ–ø—Ä–æ—Å: {question}
        –û—Ç–≤–µ—Ç:
        """.strip()

        async with httpx.AsyncClient() as client:
            response = await client.post(
                'https://llm.api.cloud.yandex.net/foundationModels/v1/completion',
                headers={
                    'Authorization': f'Api-Key {YANDEX_API_KEY}',
                    'x-folder-id': YANDEX_FOLDER_ID,
                    'Content-Type': 'application/json'
                },
                json={
                    'modelUri': YANDEX_GPT_MODEL_URI,
                    'completionOptions': {
                        'stream': False,
                        'temperature': 0.1,
                        'maxTokens': '2000'
                    },
                    'messages': [
                        {'role': 'system', 'text': '–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.'},
                        {'role': 'user', 'text': prompt}
                    ]
                },
                timeout=60.0
            )
            response.raise_for_status()
            data = response.json()
            return data['result']['alternatives'][0]['message']['text'].strip()

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Yandex GPT: {e}")
        return f"–û—à–∏–±–∫–∞ Yandex GPT: {str(e)}"

def search_open_sources(question: str) -> List[dict]:
    """–ü–æ–∏—Å–∫ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
    return [
        {
            "name": "MedlinePlus",
            "content": "–û–±—â–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç NIH"
        },
        {
            "name": "WHO",
            "content": "–ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
        },
        {
            "name": "CDC",
            "content": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π"
        }
    ]

# --- –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã ---
@app.get("/")
async def home():
    return {
        "status": "ok",
        "message": "Medical RAG API",
        "models_initialized": {
            "gemini_model": gemini_model is not None,
            "pinecone_index": pinecone_index is not None
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy" if gemini_model and pinecone_index else "degraded",
        "components": {
            "gemini_model": "healthy" if gemini_model else "uninitialized",
            "pinecone_index": "healthy" if pinecone_index else "uninitialized"
        }
    }

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    question = request.question.strip()
    mode = request.mode
    logger.info(f"üì• –í–æ–ø—Ä–æ—Å: {question} (—Ä–µ–∂–∏–º: {mode})")

    if not question:
        raise HTTPException(status_code=400, detail="–í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

    try:
        answer = ""
        sources = []

        if mode == "knowledge_base":
            contexts, sources = await search_knowledge_base(question)
            answer = "–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\n" + "\n\n---\n\n".join(contexts)

        elif mode == "combined_ai":
            contexts, sources = await search_knowledge_base(question)
            if contexts and contexts != ["–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"]:
                tasks = []
                if gemini_model:
                    tasks.append(generate_with_gemini(question, contexts))
                if YANDEX_API_KEY and YANDEX_FOLDER_ID:
                    tasks.append(generate_with_yandex(question, contexts))

                if tasks:
                    answers = await asyncio.gather(*tasks, return_exceptions=True)
                    answer_parts = []
                    for i, ans in enumerate(answers):
                        model_name = "Google Gemini" if i == 0 and gemini_model else "Yandex GPT"
                        if isinstance(ans, Exception):
                            answer_parts.append(f"‚ùå –û—à–∏–±–∫–∞ {model_name}: {str(ans)}")
                        else:
                            answer_parts.append(f"‚úÖ {model_name}:\n{ans}")
                    answer = "\n\n---\n\n".join(answer_parts)
                else:
                    answer = "–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –ò–ò –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞"
            else:
                answer = "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                sources = ["–°–∏—Å—Ç–µ–º–∞"]

        elif mode == "unified_ai":
            tasks = []
            if gemini_model:
                tasks.append(generate_with_gemini(question, []))
            if YANDEX_API_KEY and YANDEX_FOLDER_ID:
                tasks.append(generate_with_yandex(question, []))

            if tasks:
                answers = await asyncio.gather(*tasks, return_exceptions=True)
                answer_parts = []
                for i, ans in enumerate(answers):
                    model_name = "Google Gemini" if i == 0 and gemini_model else "Yandex GPT"
                    if isinstance(ans, Exception):
                        answer_parts.append(f"‚ùå –û—à–∏–±–∫–∞ {model_name}: {str(ans)}")
                    else:
                        answer_parts.append(f"‚úÖ {model_name}:\n{ans}")
                answer = "\n\n---\n\n".join(answer_parts)
                sources = ["Google Gemini", "Yandex GPT"]
            else:
                answer = "–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –ò–ò –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞"
                sources = ["–°–∏—Å—Ç–µ–º–∞"]

        elif mode == "open_sources":
            open_sources = search_open_sources(question)
            sources = [src["name"] for src in open_sources]

            open_contexts = [f"–ò—Å—Ç–æ—á–Ω–∏–∫: {src['name']}\n{src['content']}" for src in open_sources]
            context_text = "\n\n".join(open_contexts)

            tasks = []
            if gemini_model:
                tasks.append(generate_with_gemini(question, [context_text]))
            if YANDEX_API_KEY and YANDEX_FOLDER_ID:
                tasks.append(generate_with_yandex(question, [context_text]))

            if tasks:
                answers = await asyncio.gather(*tasks, return_exceptions=True)
                answer_parts = []
                for i, ans in enumerate(answers):
                    model_name = "Google Gemini" if i == 0 and gemini_model else "Yandex GPT"
                    if isinstance(ans, Exception):
                        answer_parts.append(f"‚ùå –û—à–∏–±–∫–∞ {model_name}: {str(ans)}")
                    else:
                        answer_parts.append(f"‚úÖ {model_name}:\n{ans}")
                answer = "\n\n---\n\n".join(answer_parts)
            else:
                answer = f"üìö –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {context_text}"

        else:
            raise HTTPException(status_code=400, detail="–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ä–µ–∂–∏–º")

        logger.info("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
        return AnswerResponse(
            question=question,
            answer=answer,
            sources=sources,
            mode=mode
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")

# --- –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ ---
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É {port}...")
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)
