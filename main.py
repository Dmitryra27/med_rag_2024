# main.py - Medical RAG API —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Pinecone llama-text-embed-v2
import os
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pinecone import Pinecone
from pydantic import BaseModel
from typing import List, Optional
import vertexai
from vertexai.generative_models import GenerativeModel
import pinecone
import httpx
import json
import traceback
import asyncio
from datetime import datetime, timezone

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(
    level=logging.INFO, # –£—Ä–æ–≤–µ–Ω—å INFO –¥–ª—è production, DEBUG –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ ---
gemini_model = None
pinecone_index = None
pinecone_client = None # –ù–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç Pinecone –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
INITIALIZATION_ERROR = None # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
PROJECT_ID = os.environ.get("PROJECT_ID", "ai-project-26082025")
REGION = os.environ.get("REGION", "us-central1")
PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "medical-knowledge")

YANDEX_API_KEY = os.environ.get("YANDEX_API_KEY")
YANDEX_FOLDER_ID = os.environ.get("YANDEX_FOLDER_ID",'b1gatnfegvh5a9a5iovu')
YANDEX_GPT_MODEL_URI = f"gpt://{YANDEX_FOLDER_ID}/yandexgpt/latest" if YANDEX_FOLDER_ID else None
MAX_CONTEXT_LENGTH = int(os.environ.get("MAX_CONTEXT_LENGTH", 5000)) # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

# --- Lifespan handler –¥–ª—è FastAPI ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global gemini_model, pinecone_index, pinecone_client, INITIALIZATION_ERROR
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Medical RAG API...")

    try:
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vertex AI (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤)
        logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vertex AI: project={PROJECT_ID}, location={REGION}")
        vertexai.init(project=PROJECT_ID, location=REGION)
        logger.info("‚úÖ Vertex AI –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")

        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Google Gemini (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤)
        logger.info("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ gemini-2.5-pro...")
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ 2.5-pro, –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º 1.5-pro
            gemini_model = GenerativeModel("gemini-2.5-pro")
        except Exception as e25:
            logger.info("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å gemini-2.5-pro –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–±—É–µ–º gemini-2.5-pro...")
            try:
                gemini_model = GenerativeModel("gemini-2.5-pro")
            except Exception as e15:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ Gemini 2.5-pro: {e15}")
                gemini_model = None
        if gemini_model:
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Gemini –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å Gemini.")

        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pinecone (–¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        logger.info("üîó –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pinecone...")
        if not PINECONE_API_KEY:
            raise ValueError("‚ùå –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PINECONE_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
        if not PINECONE_INDEX_NAME:
            raise ValueError("‚ùå –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PINECONE_INDEX_NAME –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")

        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç Pinecone
        pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY)
        pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –∏–Ω–¥–µ–∫—Å—É
        pinecone_index = pinecone_client.Index(PINECONE_INDEX_NAME)
        index = pinecone_client.Index(PINECONE_INDEX_NAME)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        index_stats = pinecone_index.describe_index_stats()
        logger.info(f"‚úÖ Pinecone –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ò–Ω–¥–µ–∫—Å '{PINECONE_INDEX_NAME}' —Å–æ–¥–µ—Ä–∂–∏—Ç {index_stats.get('total_vector_count', 0)} –≤–µ–∫—Ç–æ—Ä–æ–≤.")
        logger.info(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {index_stats.get('dimension', 'N/A')}")
        logger.info(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: llama-text-embed-v2")

        INITIALIZATION_ERROR = None # –°–±—Ä–æ—Å –æ—à–∏–±–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏

    except Exception as e:
        error_msg = f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}"
        logger.error(error_msg)
        logger.error(f"   Traceback: {traceback.format_exc()}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫—É, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –µ—ë –≤ /health
        INITIALIZATION_ERROR = str(e)
        # –ù–µ –±—Ä–æ—Å–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—Å—Ç–∏–ª–æ—Å—å, –Ω–æ —Å –æ—à–∏–±–∫–æ–π

    logger.info("üü¢ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–æ–≤.")
    yield
    logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")

# --- –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI —Å lifespan handler ---
app = FastAPI(
    lifespan=lifespan,
    title="Medical RAG API",
    version="3.4.0-Pinecone-Llama-Only",
    description="API –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ —Å RAG, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å Llama –æ—Ç Pinecone"
)

# --- Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –í production —É–∫–∞–∂–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ origins.
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π ---
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Å–µ—Ö –Ω–µ –ø–æ–π–º–∞–Ω–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π."""
    logger.error(f"üö® –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "detail": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
            "error_type": type(exc).__name__,
            # –í production –ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–π—Ç–µ traceback –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é!
            # "debug_info": traceback.format_exc()
        },
    )

# --- –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö ---
class QuestionRequest(BaseModel):
    question: str
    mode: str = "knowledge_base" # 'knowledge_base', 'combined_ai'

class AnswerResponse(BaseModel):
    question: str
    answer: str
    sources: List[str]
    mode: str

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
# main.py (–Ω–∞ Render) - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ search_knowledge_base

# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç –∫–ª–∏–µ–Ω—Ç–∞ Pinecone
# –û–±—ã—á–Ω–æ —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ lifespan handler:
# pc = Pinecone(api_key=PINECONE_API_KEY)

async def search_knowledge_base(question: str, top_k: int = 3):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π Pinecone —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Llama."""
    global pinecone_index, pinecone_client

    if not pinecone_index:
        logger.warning("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–∏–Ω–¥–µ–∫—Å Pinecone –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω)")
        return ["–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"], ["–°–∏—Å—Ç–µ–º–∞"]

    try:
        logger.debug(f"üîç –ü–æ–∏—Å–∫ –≤ Pinecone –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{question}'...")

        # --- 1. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é Inference API Pinecone (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ) ---
        logger.debug("üß† –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é Pinecone Inference API...")

        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –±–ª–æ–∫–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤ –≤ asyncio.to_thread
        embedding_response = await asyncio.to_thread(
            pinecone_client.inference.embed,
            model="llama-text-embed-v2",
            inputs=[question],
            parameters={
                "input_type": "query",
                "truncate": "END"
            }

        )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞
        question_embedding = embedding_response.data[0].values
        logger.debug(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(question_embedding)})")

        # --- 2. –ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ) ---
        logger.debug("üîé –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")

        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –±–ª–æ–∫–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤ query –≤ asyncio.to_thread
        search_results = await asyncio.to_thread(
            pinecone_index.query,
            vector=question_embedding,
            top_k=top_k,
            include_metadata=True
        )

        logger.debug(f"   –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(search_results.matches)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")

        # --- 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
        contexts = []
        sources = []

        if search_results.matches:
            for match in search_results.matches:
                metadata = match.metadata or {}
                # –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –∫–ª—é—á–∏ –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–∞—à–∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ Pinecone
                text = metadata.get('content') or metadata.get('text') or metadata.get('chunk_text') or metadata.get('preview') or f"–î–æ–∫—É–º–µ–Ω—Ç ID: {match.id}"
                contexts.append(text)

                source = metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
                sources.append(source)
        else:
            logger.info("   –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.")
            contexts.append("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.")
            sources.append("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")

        logger.debug(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(contexts)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
        return contexts, sources

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Pinecone: {e}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        return ["–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"], ["Pinecone"]

async def generate_with_gemini(question: str, contexts: List[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Google Gemini."""
    if not gemini_model:
        logger.warning("–ú–æ–¥–µ–ª—å Google Gemini –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return "–ú–æ–¥–µ–ª—å Google Gemini –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"

    try:
        context_text = "\n\n".join(contexts)
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(context_text) > MAX_CONTEXT_LENGTH:
            context_text = context_text[:MAX_CONTEXT_LENGTH] + f"... (–∫–æ–Ω—Ç–µ–∫—Å—Ç —É—Å–µ—á–µ–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤)"
            logger.info(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç —É—Å–µ—á–µ–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤.")

        prompt = f"""
        –¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å, –æ–ø–∏—Ä–∞—è—Å—å –¢–û–õ–¨–ö–û –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        –û—Ç–≤–µ—á–∞–π —è—Å–Ω–æ, —Ç–æ—á–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.
        –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏: "–Ø –Ω–µ –º–æ–≥—É –¥–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –≤—Ä–∞—á—É."

        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context_text}

        –í–æ–ø—Ä–æ—Å: {question}
        –û—Ç–≤–µ—Ç:
        """.strip()

        logger.debug("üí¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Google Gemini...")
        response = gemini_model.generate_content(prompt)
        answer = response.text.strip()
        logger.debug("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç Google Gemini —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")
        return answer

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Google Gemini: {e}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Google Gemini: {str(e)}"

async def generate_with_yandex(question: str, contexts: List[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Yandex GPT."""
    if not YANDEX_API_KEY or not YANDEX_FOLDER_ID:
        logger.warning("Yandex GPT –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        return "Yandex GPT –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"

    try:
        context_text = "\n\n".join(contexts)
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(context_text) > MAX_CONTEXT_LENGTH:
            context_text = context_text[:MAX_CONTEXT_LENGTH] + f"... (–∫–æ–Ω—Ç–µ–∫—Å—Ç —É—Å–µ—á–µ–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤)"
            logger.info(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç —É—Å–µ—á–µ–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤.")

        prompt = f"""
        –¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å, –æ–ø–∏—Ä–∞—è—Å—å –¢–û–õ–¨–ö–û –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        –û—Ç–≤–µ—á–∞–π —è—Å–Ω–æ, —Ç–æ—á–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.
        –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏: "–Ø –Ω–µ –º–æ–≥—É –¥–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –≤—Ä–∞—á—É."

        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context_text}

        –í–æ–ø—Ä–æ—Å: {question}
        –û—Ç–≤–µ—Ç:
        """.strip()

        logger.debug("üí¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Yandex GPT...")
        async with httpx.AsyncClient() as client:
            yandex_response = await client.post(
                'https://llm.api.cloud.yandex.net/foundationModels/v1/completion',
                headers={
                    'Authorization': f'Api-Key {YANDEX_API_KEY}',
                    'x-folder-id': YANDEX_FOLDER_ID,
                    'Content-Type': 'application/json'
                },
                json={
                    'modelUri': YANDEX_GPT_MODEL_URI,
                    'completionOptions': {
                        'stream': False,
                        'temperature': 0.1,
                        'maxTokens': '2000'
                    },
                    'messages': [
                        {'role': 'system', 'text': '–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.'},
                        {'role': 'user', 'text': prompt}
                    ]
                },
                timeout=60.0
            )
            yandex_response.raise_for_status()
            yandex_data = yandex_response.json()
            answer = yandex_data['result']['alternatives'][0]['message']['text'].strip()
            logger.debug("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç Yandex GPT —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")
            return answer

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Yandex GPT: {e}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Yandex GPT: {str(e)}"

# --- –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã API ---
@app.get("/")
async def home():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
    try:
        vector_count = 0
        dimension = 'N/A'
        if pinecone_index:
            try:
                stats = pinecone_index.describe_index_stats()
                vector_count = stats.get('total_vector_count', 0)
                dimension = stats.get('dimension', 'N/A')
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Pinecone –≤ /: {e}")
                vector_count = f"–û—à–∏–±–∫–∞: {e}"
        else:
            vector_count = "–ù–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"

        return {
            "status": "ok",
            "message": "Medical RAG API Server (Pinecone Llama Embeddings + Vertex AI)",
            "version": "3.4.0-Pinecone-Llama-Only",
            "pinecone_vectors": vector_count,
            "vector_dimension": dimension,
            "pinecone_embedding_model": "llama-text-embed-v2",
            "models_initialized": {
                "gemini_model": gemini_model is not None,
                "pinecone_index": pinecone_index is not None,
                "pinecone_client": pinecone_client is not None
            },
            "initialization_error": INITIALIZATION_ERROR
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∫–æ—Ä–Ω–µ–≤–æ–º —ç–Ω–¥–ø–æ–∏–Ω—Ç–µ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Pinecone
        pinecone_status = "uninitialized"
        vector_count = 0
        dimension = 'N/A'
        if pinecone_index is not None:
            try:
                stats = pinecone_index.describe_index_stats()
                pinecone_status = "healthy"
                vector_count = stats.get('total_vector_count', 0)
                dimension = stats.get('dimension', 'N/A')
            except Exception as e:
                pinecone_status = "unhealthy"
                logger.error(f"Health check - Pinecone error: {e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª–∏
        gemini_model_status = "healthy" if gemini_model is not None else "uninitialized"
        pinecone_client_status = "healthy" if pinecone_client is not None else "uninitialized"
        pinecone_index_status = "healthy" if pinecone_index is not None else "uninitialized"

        overall_status = "healthy"
        if not all([
            pinecone_status == "healthy",
            gemini_model_status == "healthy",
            pinecone_client_status == "healthy",
            pinecone_index_status == "healthy"
        ]):
            if INITIALIZATION_ERROR:
                overall_status = "unhealthy"
            else:
                overall_status = "degraded"

        return {
            "status": overall_status,
            "components": {
                "pinecone": {
                    "status": pinecone_status,
                    "vector_count": vector_count,
                    "dimension": dimension,
                    "embedding_model": "llama-text-embed-v2"
                },
                "gemini_model": {
                    "status": gemini_model_status
                },
                "pinecone_client": {
                    "status": pinecone_client_status
                },
                "pinecone_index": {
                    "status": pinecone_index_status
                }
            },
            "initialization_error": INITIALIZATION_ERROR,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ /health: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG.
    """
    question = request.question.strip()
    mode = request.mode
    logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –≤–æ–ø—Ä–æ—Å: {question} (—Ä–µ–∂–∏–º: {mode})")

    # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not question:
        logger.warning("–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å")
        raise HTTPException(status_code=400, detail="–í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
    if len(question) > 1000:
        logger.warning(f"–ü–æ–ª—É—á–µ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å ({len(question)} —Å–∏–º–≤–æ–ª–æ–≤)")
        raise HTTPException(status_code=400, detail="–í–æ–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–∞–∫—Å–∏–º—É–º 1000 —Å–∏–º–≤–æ–ª–æ–≤)")

    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    if not gemini_model and not (YANDEX_API_KEY and YANDEX_FOLDER_ID):
        error_msg = "–°–µ—Ä–≤–∏—Å –Ω–µ –≥–æ—Ç–æ–≤: –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)
    if not pinecone_index or not pinecone_client:
        error_msg = "–°–µ—Ä–≤–∏—Å –Ω–µ –≥–æ—Ç–æ–≤: –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

    try:
        # 3. –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π (–≤—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)
        contexts, sources = await search_knowledge_base(question, top_k=3)
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(contexts)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        answer = ""
        if mode == "knowledge_base":
            # –¢–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            answer = "–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\n" + "\n\n---\n\n".join(contexts)
        elif mode == "combined_ai":
            # –ü–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            tasks = []
            models_used = []

            # Google Gemini
            if gemini_model:
                tasks.append(generate_with_gemini(question, contexts))
                models_used.append("Google Gemini")
            else:
                logger.warning("Google Gemini –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

            # Yandex GPT
            if YANDEX_API_KEY and YANDEX_FOLDER_ID:
                tasks.append(generate_with_yandex(question, contexts))
                models_used.append("Yandex GPT")
            else:
                logger.warning("Yandex GPT –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∫–ª—é—á–∏ API")

            if not tasks:
                raise HTTPException(status_code=500, detail="–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –ò–ò –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")

            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            answers = await asyncio.gather(*tasks, return_exceptions=True)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã
            combined_parts = []
            for model_name, ans in zip(models_used, answers):
                if isinstance(ans, Exception):
                    logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è {model_name}: {ans}")
                    combined_parts.append(f"–û—à–∏–±–∫–∞ –æ—Ç {model_name}: {ans}")
                else:
                    combined_parts.append(f"–û—Ç–≤–µ—Ç –æ—Ç {model_name}:\n{ans}")

            answer = "\n\n---\n\n".join(combined_parts)

        else:
            raise HTTPException(status_code=400, detail="–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ä–µ–∂–∏–º")

        # 5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç
        logger.info("‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
        return AnswerResponse(
            question=question,
            answer=answer,
            sources=sources,
            mode=mode
        )

    except HTTPException:
        # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º HTTPException –∫–∞–∫ –µ—Å—Ç—å
        raise
    except Exception as e:
        logger.error(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ /ask: {e}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}")

# --- –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ ---
if __name__ == "__main__":
    import uvicorn
    from datetime import datetime
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π Cloud Run/Render
    port = int(os.environ.get("PORT", 8080))
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ Uvicorn –Ω–∞ –ø–æ—Ä—Ç—É {port}...")
    # –í–ê–ñ–ù–û: host –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å "0.0.0.0" –¥–ª—è Cloud Run/Render
    uvicorn.run(app, host="0.0.0.0", port=port)
